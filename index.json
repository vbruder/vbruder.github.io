[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a doctoral researcher at the University of Stuttgart Visualization Research Center (VISUS) and part of the collaborative research center SFB-TRR 161 on quantitative methods for visual computing. I develop new methods to assess, model, and predict performance of visual computing systems. Thereby, I focus on applications from the domain of scientific visualization.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1557737128,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://vbruder.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"I\u0026rsquo;m a doctoral researcher at the University of Stuttgart Visualization Research Center (VISUS) and part of the collaborative research center SFB-TRR 161 on quantitative methods for visual computing. I develop new methods to assess, model, and predict performance of visual computing systems. Thereby, I focus on applications from the domain of scientific visualization.","tags":null,"title":"Valentin Bruder","type":"author"},{"authors":["Valentin Bruder"],"categories":null,"content":"","date":1571565282,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561384252,"objectID":"40223f6d6959a148596421b8ba63d3b7","permalink":"https://vbruder.github.io/talk/2019-scivis/","publishdate":"2019-05-12T11:54:42+02:00","relpermalink":"/talk/2019-scivis/","section":"talk","summary":"Presentation of our Transactions on Visualization and Computer Graphics paper.","tags":[],"title":"On Evaluating Runtime Performance of Interactive Visualizations","type":"talk"},{"authors":["Valentin Bruder","Houssem Ben Lahmar","Marcel Hlawatsch","Steffen Frey","Michael Burch","Daniel Weiskopf","Melanie Herschel","Thomas Ertl"],"categories":null,"content":"","date":1561932000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561384252,"objectID":"9f1c15c4464e0f8a5276c346382d4177","permalink":"https://vbruder.github.io/publication/bruder-lhfbwhe-19/","publishdate":"2019-07-01T00:00:00+02:00","relpermalink":"/publication/bruder-lhfbwhe-19/","section":"publication","summary":"We present an approach for the visualization and interactive analysis of dynamic graphs that contain a large number of time steps. A specific focus is put on the support of analyzing temporal aspects in the data. Central to our approach is a static, volumetric representation of the dynamic graph based on the concept of space-time cubes that we create by stacking the adjacency matrices of all time steps. The use of GPU-accelerated volume rendering techniques allows us to render this representation interactively. We identified four classes of analytics methods as being important for the analysis of large and complex graph data, which we discuss in detail: data views, aggregation and filtering, comparison, and evolution provenance. Implementations of the respective methods are presented in an integrated application, enabling interactive exploration and analysis of large graphs. We demonstrate the applicability, usefulness, and scalability of our approach by presenting two examples for analyzing dynamic graphs. Furthermore, we let visualization experts evaluate our analytics approach.","tags":null,"title":"Volume-Based Large Dynamic Graph Analytics Supported by Evolution Provenance","type":"publication"},{"authors":["Kuno Kurzhals"],"categories":null,"content":"","date":1561622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561384252,"objectID":"e6d9a5546e4220ad1aac0c78e6f3109d","permalink":"https://vbruder.github.io/talk/2019-etra/","publishdate":"2019-05-11T11:54:31+02:00","relpermalink":"/talk/2019-etra/","section":"talk","summary":"Presentation of our ETRA 2019 paper.","tags":[],"title":"Space-Time Volume Visualization of Gaze and Stimulus","type":"talk"},{"authors":["Valentin Bruder"],"categories":null,"content":"","date":1559725800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561384252,"objectID":"f1cc5e8b226124acd657c2487a5cabe0","permalink":"https://vbruder.github.io/talk/2019-eurovis/","publishdate":"2019-05-11T11:54:31+02:00","relpermalink":"/talk/2019-eurovis/","section":"talk","summary":"Presentation of our EuroVis 2019 short paper.","tags":[],"title":"Voronoi-Based Foveated Volume Rendering","type":"talk"},{"authors":["Valentin Bruder","Kuno Kurzhals","Steffen Frey","Daniel Weiskopf","Thomas Ertl"],"categories":null,"content":"","date":1559340000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557741100,"objectID":"3d0894f3025830541c96388112b250c2","permalink":"https://vbruder.github.io/publication/dblp-confetra-bruder-kfwe-19/","publishdate":"2019-06-01T00:00:00+02:00","relpermalink":"/publication/dblp-confetra-bruder-kfwe-19/","section":"publication","summary":"We present a method for the spatio-temporal analysis of gaze data from multiple participants in the context of a video stimulus. For such data, an overview of the recorded patterns is important to identify common viewing behavior (such as attentional synchrony) and outliers. We adopt the approach of space-time cube visualization, which extends the spatial dimensions of the stimulus by time as the third dimension. Previous work mainly handled eye-tracking data in the space-time cube as point cloud, providing no information about the stimulus context. This paper presents a novel visualization technique that combines gaze data, a dynamic stimulus, and optical flow with volume rendering to derive an overview of the data with contextual information. With specifically designed transfer functions, we emphasize different data aspects, making the visualization suitable for explorative analysis and for illustrative support of statistical findings alike.","tags":null,"title":"Space-time volume visualization of gaze and stimulus","type":"publication"},{"authors":["Valentin Bruder","Christoph Schulz","Ruben Bauer","Steffen Frey","Daniel Weiskopf","Thomas Ertl"],"categories":null,"content":"","date":1559340000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561384252,"objectID":"b8024263bc6365a80927fd70cb9c43d9","permalink":"https://vbruder.github.io/publication/bruder-sbfwe-19/","publishdate":"2019-06-01T00:00:00+02:00","relpermalink":"/publication/bruder-sbfwe-19/","section":"publication","summary":"Foveal vision is located in the center of the field of view with a rich impression of detail and color, whereas peripheral vision occurs on the side with more fuzzy and colorless perception. This visual acuity fall-off can be used to achieve higher frame rates by adapting rendering quality to the human visual system. Volume raycasting has unique characteristics, preventing a direct transfer of many traditional foveated rendering techniques. We present an approach that utilizes the visual acuity fall-off to accelerate volume rendering based on Linde-Buzo-Gray sampling and natural neighbor interpolation. First, we measure gaze using a stationary 1200 Hz eye-tracking system. Then, we adapt our sampling and reconstruction strategy to that gaze. Finally, we apply a temporal smoothing filter to attenuate undersampling artifacts since peripheral vision is particularly sensitive to contrast changes and movement. Our approach substantially improves rendering performance with barely perceptible changes in visual quality. We demonstrate the usefulness of our approach through performance measurements on various data sets.","tags":null,"title":"Voronoi-Based Foveated Volume Rendering","type":"publication"},{"authors":null,"categories":null,"content":"","date":1552321344,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552330379,"objectID":"0e198f3ea1a6f130d9ee03002e487211","permalink":"https://vbruder.github.io/project/trrojan/","publishdate":"2019-03-11T17:22:24+01:00","relpermalink":"/project/trrojan/","section":"project","summary":"We developed the TRRojan framework for systematic empirical evaluation of the performance of visual computing algorithms. The TRRojan is designed to effectively carry out quantitative benchmarks in a clean, reproducible, and easy-to-use manner. It uses a plugin architecture for simple extensibility with additional applications. The framework is published as open source software (MIT licence) and the data we obtained during measurements, as well as additional information can be found on the [project site](http://trrojan.visus.uni-stuttgart.de).","tags":[],"title":"TRRojan","type":"project"},{"authors":["Valentin Bruder","Christoph MÃ¼ller","Steffen Frey","Thomas Ertl"],"categories":null,"content":"","date":1548975600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557741100,"objectID":"b5525eddd192ca3beb315cc9c5a059d5","permalink":"https://vbruder.github.io/publication/8637795/","publishdate":"2019-02-01T00:00:00+01:00","relpermalink":"/publication/8637795/","section":"publication","summary":"As our field matures, evaluation of visualization techniques has extended from reporting runtime performance to studying user behavior. Consequently, many methodologies and best practices for user studies have evolved. While maintaining interactivity continues to be crucial for the exploration of large data sets, no similar methodological foundation for evaluating runtime performance has been developed. Our analysis of 50 recent visualization papers on new or improved techniques for rendering volumes or particles indicates that   only a very limited set of parameters like different data sets, camera paths, viewport sizes, and GPUs are investigated, which make comparison with other techniques or generalization to other parameter ranges at least questionable. To derive a deeper understanding of qualitative runtime behavior and quantitative parameter dependencies, we developed a framework for the most exhaustive performance evaluation of volume and particle visualization techniques that we are aware of, including millions of measurements on ten different GPUs. This paper reports on our insights from statistical analysis of this data, discussing independent and linear parameter behavior and non-obvious effects. We give recommendations for best practices when evaluating runtime performance of scientific visualization applications, which can serve as a starting point for more elaborate models of performance quantification. ","tags":["Rendering (computer graphics);Performance evaluation;Data visualization;Computational modeling;Runtime;Benchmark testing;Best practices;Performance evaluation;scientific visualization;volume rendering;particle rendering"],"title":"On Evaluating Runtime Performance of Interactive Visualizations","type":"publication"},{"authors":["Florian FrieÃ","Mathias Landwehr","Valentin Bruder","Steffen Frey","Thomas Ertl"],"categories":null,"content":"","date":1514761200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554223620,"objectID":"74e2d1f403dd8df3e114a59294eda6e5","permalink":"https://vbruder.github.io/publication/dblp-confldav-friess-lbfe-18/","publishdate":"2018-01-01T00:00:00+01:00","relpermalink":"/publication/dblp-confldav-friess-lbfe-18/","section":"publication","summary":"We present an approach that dynamically adapts encoder settings for image tiles to yield the best possible quality for a given bandwidth. This reduces the overall size of the image while preserving details. Our application determines the encoding settings in two steps. In the first step, we predict the quality and size of the tiles for different encoding settings using a convolutional neural network. In the second step, we assign the optimal encoder setting to each tile, so that the overall size of the image is lower than a predetermined threshold. Commonly, for tiles that contain complicated structures, a high quality setting is used in order to prevent major information loss, while quality settings are lowered for others to keep the size below the threshold. We demonstrate that we can reduce the overall size of the image while preserving the details in areas of interest using the example of both particle and volume visualization applications.","tags":null,"title":"Adaptive Encoder Settings for Interactive Remote Visualisation on High-Resolution Displays","type":"publication"},{"authors":["Valentin Bruder","Marcel Hlawatsch","Steffen Frey","Michael Burch","Daniel Weiskopf","Thomas Ertl"],"categories":null,"content":"","date":1514761200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554223620,"objectID":"bdd4731e3f384775b9b69602d6bfbb40","permalink":"https://vbruder.github.io/publication/dblp-confiv-bruder-hfbwe-18/","publishdate":"2018-01-01T00:00:00+01:00","relpermalink":"/publication/dblp-confiv-bruder-hfbwe-18/","section":"publication","summary":"We present an approach for interactively analyzing large dynamic graphs consisting of several thousand time steps with a particular focus on temporal aspects. we employ a static representation of the time-varying graph based on the concept of space-time cubes, i.e., we create a volumetric representation of the graph by stacking the adjacency matrices of each of its time steps. To achieve an efficient analysis of complex data, we discuss three classes of analytics methods of particular importance in this context: data views, aggregation and filtering, and comparison. For these classes, we present a GPU-based implementation of respective analysis methods that enable the interactive analysis of large graphs. We demonstrate the utility as well as the scalability of our approach by presenting application examples for analyzing different time-varying data sets.","tags":null,"title":"Volume-Based Large Dynamic Graph Analytics","type":"publication"},{"authors":["Moritz Heinemann","Valentin Bruder","Steffen Frey","Thomas Ertl"],"categories":null,"content":"","date":1483225200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557741100,"objectID":"c9c1ef06641f9d8d6484a3be5af86ef2","permalink":"https://vbruder.github.io/publication/dblp-confvissym-heinemann-bfe-17/","publishdate":"2017-01-01T00:00:00+01:00","relpermalink":"/publication/dblp-confvissym-heinemann-bfe-17/","section":"publication","summary":"Power efficiency is one of the most important factors for the development of compute-intensive applications in the mobile domain. In this work, we evaluate and discuss the power consumption of a direct volume rendering app based on raycasting on a mobile system. For this, we investigate the influence of a broad set of algorithmic parameters, which are relevant for performance and rendering quality, on the energy usage of the system. Additionally, we compare an OpenCL implementation to a variant using OpenGL. By means of a variety of examples, we demonstrate that numerous factors can have a significant impact on power consumption. In particular, we also discuss the underlying reasons for the respective effects.","tags":null,"title":"Power Efficiency of Volume Raycasting on Mobile Devices","type":"publication"},{"authors":["Gleb Tkachev","Steffen Frey","Christoph MÃ¼ller","Valentin Bruder","Thomas Ertl"],"categories":null,"content":"","date":1483225200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557741100,"objectID":"b974f571fb5e9f2badc4e521e7219772","permalink":"https://vbruder.github.io/publication/dblp-confegpgv-tkachev-fmbe-17/","publishdate":"2017-01-01T00:00:00+01:00","relpermalink":"/publication/dblp-confegpgv-tkachev-fmbe-17/","section":"publication","summary":"We present our data-driven, neural network-based approach to predicting the performance of a distributed GPU volume renderer for supporting cluster equipment acquisition. On the basis of timing measurements from a single cluster as well as from individual GPUs, we are able to predict the performance gain of upgrading an existing cluster with additional or faster GPUs, or even purchasing of a new cluster with a comparable network configuration. To achieve this, we employ neural networks to capture complex performance characteristics. However, merely relying on them for the prediction would require the collection of training data on multiple clusters with different hardware, which is impractical in most cases. Therefore, we propose a two-level approach to prediction, distinguishing between node and cluster level. On the node level, we generate performance histograms on individual nodes to capture local rendering performance. These performance histograms are then used to emulate the performance of different rendering hardware for cluster-level measurement runs. Crucially, this variety allows the neural network to capture the compositing performance of a cluster separately from the rendering performance on individual nodes. Therefore, we just need a performance histogram of the GPU of interest to generate a prediction. We demonstrate the utility of our approach using different cluster configurations as well as a range of image and volume resolutions.","tags":null,"title":"Prediction of Distributed Volume Visualization Performance to Support Render Hardware Acquisition","type":"publication"},{"authors":["Valentin Bruder","Steffen Frey","Thomas Ertl"],"categories":null,"content":"","date":1483225200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557741100,"objectID":"85f38afb2691c2ef2262da5158a8510a","permalink":"https://vbruder.github.io/publication/dblp-journalsvi-bruder-fe-17/","publishdate":"2017-01-01T00:00:00+01:00","relpermalink":"/publication/dblp-journalsvi-bruder-fe-17/","section":"publication","summary":"We present an integrated approach for real-time performance prediction of volume raycasting that we employ for load balancing and sampling resolution tuning. In volume rendering, the usage of acceleration techniques such as empty space skipping and early ray termination, among others, can cause significant variations in rendering performance when users adjust the camera configuration or transfer function. These variations in rendering times may result in unpleasant effects such as jerky motions or abruptly reduced responsiveness during interactive exploration. To avoid those effects, we propose an integrated approach to adapt rendering parameters according to performance needs. We assess performance-relevant data on-the-fly, for which we propose a novel technique to estimate the impact of early ray termination. On the basis of this data, we introduce a hybrid model, to achieve accurate predictions with minimal computational footprint. Our hybrid model incorporates aspects from analytical performance modeling and machine learning, with the goal to combine their respective strengths. We show the applicability of our prediction model for two different use cases: (1) to dynamically steer the sampling density in object and/or image space and (2) to dynamically distribute the workload among several different parallel computing devices. Our approach allows to reliably meet performance requirements such as a user-defined frame rate, even in the case of sudden large changes to the transfer function or the camera orientation.","tags":null,"title":"Prediction-based load balancing and resolution tuning for interactive volume raycasting","type":"publication"},{"authors":["Valentin Bruder","Steffen Frey","Thomas Ertl"],"categories":null,"content":"","date":1451602800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552319512,"objectID":"cae2a7f99ea3652a046b86ececb62a0e","permalink":"https://vbruder.github.io/publication/dblp-confsiggraph-bruder-fe-16/","publishdate":"2016-01-01T00:00:00+01:00","relpermalink":"/publication/dblp-confsiggraph-bruder-fe-16/","section":"publication","summary":"We present an integrated approach for the real-time performance prediction and tuning of volume raycasting. The usage of empty space skipping and early ray termination, among others, can induce significant variations in performance when camera configuration and transfer functions are adjusted. For interactive exploration, this can result in various unpleasant effects like abruptly reduced responsiveness or jerky motions. To overcome those effects, we propose an integrated approach to accelerate the rendering and assess performance-relevant data on-the-fly, including a new technique to estimate the impact of early ray termination. On this basis, we introduce a hybrid model, to achieve accurate predictions with only minimal computational footprint. Our hybrid model incorporates both aspects from analytical performance modeling and machine learning, with the goal to combine their respective strengths. Using our model, we dynamically steer the sampling density along rays with our automatic tuning technique. This approach allows to reliably meet performance requirements like a fixed frame rate, even in the case of large sudden changes to the transfer function or the camera. We finally demonstrate the accuracy and utility of our approach by means of a variety of different volume data sets and interaction sequences.","tags":null,"title":"Real-time performance prediction and tuning for interactive volume raycasting","type":"publication"}]